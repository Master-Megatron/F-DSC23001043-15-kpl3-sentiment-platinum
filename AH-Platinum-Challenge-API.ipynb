{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d60e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "from flask import request\n",
    "from flasgger import Swagger, LazyString, LazyJSONEncoder\n",
    "from flasgger import swag_from\n",
    "from flask import Flask, jsonify\n",
    "import sqlite3\n",
    "import demoji\n",
    "import emoji\n",
    "from unidecode import unidecode\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe603612",
   "metadata": {},
   "source": [
    "## FUNGSI-FUNGSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d60ce8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fungsi untuk menghilangkan karakter\n",
    "def clean_karakter(texts):\n",
    "    tanpa_hex = [re.sub(r'\\\\x..', ' ', text) for text in texts]\n",
    "    tanpa_backslash = [re.sub(r'\\\\', ' ', text) for text in tanpa_hex]\n",
    "    tanpa_newline = [re.sub(r'\\n', ' ', text) for text in tanpa_backslash]\n",
    "    non_ascii = [re.sub(r'[^\\x00-\\x7F]+', ' ', text) for text in tanpa_newline]   \n",
    "    non_url = [re.sub(r'https?://\\S+|www\\.\\S+', ' ', text) for text in non_ascii]\n",
    "    non_whitespace = [re.sub(r'\\s+', ' ', text) for text in non_url]\n",
    "    only_text = [re.sub(r'[^a-zA-Z0-9\\s]', ' ', text) for text in non_whitespace]\n",
    "    lower_text = [re.sub(r'[A-Z]', lambda x: x.group(0).lower(), text) for text in only_text]\n",
    "    tanpa_kata_user = [re.sub(r'\\buser\\b', ' ', text) for text in lower_text]\n",
    "    tanpa_kata_RT = [re.sub(r'\\brt\\b', ' ', text) for text in tanpa_kata_user]\n",
    "    cleaned_text_new = [re.sub(r'\\s+', ' ', text).strip() for text in tanpa_kata_RT]\n",
    "    return cleaned_text_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_abusive(text):\n",
    "    df_abusive = pd.read_csv('asset-challenges/abusive.csv')\n",
    "    abusive_words = df_abusive['ABUSIVE'].tolist()\n",
    "    abusive_words_lower = [word.lower() for word in abusive_words]\n",
    "    for kata in abusive_words_lower:\n",
    "        text = re.sub(r'\\b' + re.escape(kata) + r'\\b', '', text, flags=re.IGNORECASE)\n",
    "    cleaned_text = re.sub(' +', ' ', text).strip()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bba4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_kata_alay(text, df_kamusalay):\n",
    "    words = text.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in df_kamusalay['kata_alay'].values:\n",
    "            new_word = df_kamusalay.loc[df_kamusalay['kata_alay'] == words[i], 'kata_baku'].values[0]\n",
    "            words[i] = new_word\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9d9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6c0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "317ab698",
   "metadata": {},
   "source": [
    "# PREPARE MEMBUAT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "126a8579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_kar(text):\n",
    "    text = re.sub(r'\\\\x..', ' ', text)\n",
    "    text = re.sub(r'\\\\', ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'[A-Z]', lambda x: x.group(0).lower(), text)\n",
    "    text = re.sub(r'\\buser\\b', ' ', text)\n",
    "    text = re.sub(r'\\brt\\b', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c84e5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# Definisikan parameter untuk feature extraction\n",
    "max_features = 100000\n",
    "tokenizer = Tokenizer (num_words=max_features, split = ' ', lower = True)\n",
    "\n",
    "# Definisikan label untuk sentiment\n",
    "sentiment = ['negative', 'neutral','positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5251bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('platinum/resources_of_lstm/x_pad_sequences.pickle', 'rb')\n",
    "feature_file_from_lstm = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4291e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_from_lstm = load_model ('platinum/model_of_lstm/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05034a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d264c90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d77ea57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akung\\anaconda3\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelBinarizer from version 1.0.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\akung\\anaconda3\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MLPClassifier from version 1.0.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Memuat kembali model dari berkas\n",
    "with open(\"platinum/model_of_nn/model_mlp_classifier.pkl\", \"rb\") as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8581689c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akung\\anaconda3\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator CountVectorizer from version 1.0.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Buka file untuk dibaca dalam mode biner (\"rb\")\n",
    "with open(\"platinum/feature.p\", \"rb\") as file:\n",
    "    # Mengembalikan objek yang disimpan menggunakan pickle\n",
    "    count_vect = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620ccab",
   "metadata": {},
   "source": [
    "# PREPARE MEMBUAT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033c5bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akung\\AppData\\Local\\Temp\\ipykernel_22296\\2218998922.py:3: DeprecationWarning: 'app.json_encoder' is deprecated and will be removed in Flask 2.3. Customize 'app.json_provider_class' or 'app.json' instead.\n",
      "  app.json_encoder = LazyJSONEncoder\n",
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [01/Mar/2024 19:38:18] \"GET / HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [01/Mar/2024 19:38:18] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [01/Mar/2024 19:38:22] \"GET /api/ HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Mar/2024 19:38:22] \"GET /flasgger_static/swagger-ui-bundle.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [01/Mar/2024 19:38:22] \"GET /flasgger_static/swagger-ui-standalone-preset.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [01/Mar/2024 19:38:22] \"GET /flasgger_static/lib/jquery.min.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [01/Mar/2024 19:38:22] \"GET /flasgger_static/swagger-ui.css HTTP/1.1\" 304 -\n",
      "ERROR:root:jsonify failure; defaulting to json.dumps\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 581, in load_from_file\n",
      "    enc = detect_by_bom(swag_path)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 616, in detect_by_bom\n",
      "    with open(path, 'rb') as f:\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\akung\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_22296\\\\api/neural_network.yml'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 590, in load_from_file\n",
      "    enc = detect_by_bom(swag_path)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 616, in detect_by_bom\n",
      "    with open(path, 'rb') as f:\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\akung\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_22296\\\\api/neural_network.yml'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\base.py\", line 164, in get\n",
      "    return jsonify(self.loader())\n",
      "                   ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\base.py\", line 438, in get_apispecs\n",
      "    specs = get_specs(\n",
      "            ^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 180, in get_specs\n",
      "    doc_summary, doc_description, doc_swag = parse_docstring(\n",
      "                                             ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 646, in parse_docstring\n",
      "    full_doc = load_from_file(swag_paths[key], swag_type)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 604, in load_from_file\n",
      "    if package_spec.has_location:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'has_location'\n",
      "ERROR:__main__:Exception on /api.json [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 581, in load_from_file\n",
      "    enc = detect_by_bom(swag_path)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 616, in detect_by_bom\n",
      "    with open(path, 'rb') as f:\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\akung\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_22296\\\\api/neural_network.yml'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 590, in load_from_file\n",
      "    enc = detect_by_bom(swag_path)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 616, in detect_by_bom\n",
      "    with open(path, 'rb') as f:\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\akung\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_22296\\\\api/neural_network.yml'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\base.py\", line 164, in get\n",
      "    return jsonify(self.loader())\n",
      "                   ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\base.py\", line 438, in get_apispecs\n",
      "    specs = get_specs(\n",
      "            ^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 180, in get_specs\n",
      "    doc_summary, doc_description, doc_swag = parse_docstring(\n",
      "                                             ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 646, in parse_docstring\n",
      "    full_doc = load_from_file(swag_paths[key], swag_type)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 604, in load_from_file\n",
      "    if package_spec.has_location:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'has_location'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 581, in load_from_file\n",
      "    enc = detect_by_bom(swag_path)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 616, in detect_by_bom\n",
      "    with open(path, 'rb') as f:\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\akung\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_22296\\\\api/neural_network.yml'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 590, in load_from_file\n",
      "    enc = detect_by_bom(swag_path)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 616, in detect_by_bom\n",
      "    with open(path, 'rb') as f:\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\akung\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_22296\\\\api/neural_network.yml'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 2525, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1822, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1820, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1796, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flask\\views.py\", line 107, in view\n",
      "    return current_app.ensure_sync(self.dispatch_request)(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flask\\views.py\", line 188, in dispatch_request\n",
      "    return current_app.ensure_sync(meth)(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\base.py\", line 168, in get\n",
      "    specs = json.dumps(self.loader())\n",
      "                       ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\base.py\", line 438, in get_apispecs\n",
      "    specs = get_specs(\n",
      "            ^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 180, in get_specs\n",
      "    doc_summary, doc_description, doc_swag = parse_docstring(\n",
      "                                             ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 646, in parse_docstring\n",
      "    full_doc = load_from_file(swag_paths[key], swag_type)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\akung\\anaconda3\\Lib\\site-packages\\flasgger\\utils.py\", line 604, in load_from_file\n",
      "    if package_spec.has_location:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'has_location'\n",
      "127.0.0.1 - - [01/Mar/2024 19:38:22] \"GET /api.json HTTP/1.1\" 500 -\n",
      "INFO:werkzeug:127.0.0.1 - - [01/Mar/2024 19:38:22] \"\u001b[35m\u001b[1mGET /api.json HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "app.json_encoder = LazyJSONEncoder\n",
    "swagger_template = {\n",
    "    \"info\": {\n",
    "        \"title\":  \"API Documentation for Data Processing and Modeling\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"description\": \"Dokumentasi API untuk Data Processing dan Modeling\"\n",
    "    },\n",
    "    \"host\": \"127.0.0.1:5000\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# swagger_template = dict(\n",
    "# info = {\n",
    "#     'title': LazyString(lambda: 'API Documentation for Deep Learning'),\n",
    "#     'version': LazyString(lambda: '1.0.0'),\n",
    "#     'description': LazyString(lambda: 'Dokumentasi API untuk Deep Learning'),\n",
    "#     },\n",
    "#     host = LazyString(lambda: request.host)\n",
    "# )\n",
    "\n",
    "swagger_config = {\n",
    "    \"headers\": [],\n",
    "    \"specs\": [\n",
    "        {\n",
    "            \"endpoint\": 'api',\n",
    "            \"route\": '/api.json',\n",
    "        }\n",
    "    ],\n",
    "    \"static_url_path\": \"/flasgger_static\",\n",
    "    \"swagger_ui\": True,\n",
    "    \"specs_route\": \"/api/\"\n",
    "}\n",
    "\n",
    "\n",
    "swagger = Swagger(app, template=swagger_template,             \n",
    "                  config=swagger_config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input kalimat\n",
    "@swag_from(\"api/neural_network.yml\", methods=['POST'])\n",
    "@app.route('/neural-network', methods=['POST'])\n",
    "def text_processing_nn():\n",
    "\n",
    "    original_text = request.form.get('text')\n",
    "\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', original_text)\n",
    "    cleaned_text = re.sub(' +', ' ', cleaned_text).strip()\n",
    "    \n",
    "#     cleaned_text = replace_kata_alay(cleaned_text, df_kamusalay)\n",
    "#     cleaned_text = remove_abusive(cleaned_text)\n",
    "#     insert_data(text, cleaned_text)\n",
    "\n",
    "    # Mengubah teks menjadi vektor fitur\n",
    "    text = count_vect.transform([cleaned_text])\n",
    "\n",
    "    # Menggunakan model untuk memprediksi sentimen teks\n",
    "    result = loaded_model.predict(text)[0]\n",
    "\n",
    "    \n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Hasil Sentimen analisis menggunakan Neural Network\",\n",
    "        'text': cleaned_text,\n",
    "        'sentiment': result\n",
    "    }\n",
    "\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "\n",
    "@swag_from(\"api/lstm.yml\", methods=['POST'])\n",
    "@app.route('/lstm', methods=['POST'])\n",
    "def text_processing_lstm():\n",
    "\n",
    "    original_text = request.form.get('text')\n",
    "\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', original_text)\n",
    "    cleaned_text = re.sub(' +', ' ', cleaned_text).strip()\n",
    "    \n",
    "#     cleaned_text = replace_kata_alay(cleaned_text, df_kamusalay)\n",
    "#     cleaned_text = remove_abusive(cleaned_text)\n",
    "#     insert_data(text, cleaned_text)\n",
    "\n",
    "    feature = tokenizer.texts_to_sequences(text)\n",
    "    feature = pad_sequences(feature, maxlen=feature_file_from_lstm.shape[1])\n",
    "    \n",
    "    prediction = model_file_from_lstm.predict(feature)\n",
    "    get_sentiment = sentiment[np.argmax(prediction[0])]\n",
    "\n",
    "    \n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Hasil Sentimen analisis menggunakan LSTM\",\n",
    "        'data':{\n",
    "            'text': original_text,\n",
    "            'sentiment': get_sentiment\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #input file\n",
    "# @swag_from(\"C://Users/akung/api/neural_networkfile.yml\", methods=['POST'])\n",
    "# @app.route('/neural_networkfile', methods=['POST'])\n",
    "# def file_nn():\n",
    "\n",
    "#     file = request.files.getlist('file')[0]\n",
    "#     df_file = pd.read_csv(file,encoding='utf-8')\n",
    "#     df_file = df_file.drop_duplicates()\n",
    "#     alay_text = clean_karakter(df_file)\n",
    "\n",
    "    \n",
    "#     # cleaned_text_new  = [replace_kata_alay(text, df_kamusalay) for text in alay_text]\n",
    "    \n",
    "#     # for i in range(len(cleaned_text_new)):\n",
    "#     #     cleaned_text_new[i] = remove_abusive(cleaned_text_new[i])\n",
    "    \n",
    "#     for text in alay_text.iloc[:,0]:\n",
    "#         feature = tokenizer.texts_to_sequences(text)\n",
    "#         feature = pad_sequences(feature, maxlen=feature_file_from_lstm.shape[1])\n",
    "        \n",
    "#         prediction = model_file_from_lstm.predict(feature)\n",
    "#         get_sentiment = sentiment[np.argmax(prediction[0])]\n",
    "    \n",
    "#         sentiment_result.append({\n",
    "#             'text': original_text,\n",
    "#             'sentiment': get_sentiment\n",
    "\n",
    "\n",
    "#         })\n",
    "    \n",
    "\n",
    "#     json_response = {\n",
    "#                 'status_code': 200,\n",
    "#                 'description': \"Hasil Sentimen analisis menggunakan LSTM\",\n",
    "#                 'data':sentiment_result\n",
    "#             }\n",
    "        \n",
    "#     response_data = jsonify(json_response)\n",
    "#     return response_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76690945-fa6c-47ad-bf4a-2edbf936eb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddd091d6",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
